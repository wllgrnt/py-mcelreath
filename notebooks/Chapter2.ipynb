{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32bdf3f4-559f-4971-8204-ef7e5f2750a8",
   "metadata": {},
   "source": [
    "# Chapter 2\n",
    "\n",
    "Bayes' theorem applied to data given some prior:\n",
    "\n",
    "\\begin{equation}\n",
    "P(p | data ) \\propto P( data | p) \\cdot P(p)  \n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Normalised by $ P(data) = E(P( data | p) = \\int P( data | p ) P(p) dp$ so that the probability sums to 1.\n",
    "\n",
    "Terms:\n",
    "- $ P (p | data) $ - the posterior, the thing we want, the probability of our model given the data\n",
    "- $ P (data | p) $ - the likelihood\n",
    "- $ P (p) $ - the prior, the thing we choose in advance, our best guess before the data of the distribution of our parameters  of interest\n",
    "\n",
    "\n",
    "So we have a machine that conditions the priors on the data. In simple cases this can be done analytically, but in this chapter we look at three numerical techniques instead.\n",
    "\n",
    "1. Grid approximation\n",
    "2. Quadratic approximation\n",
    "3. Markov chain Monte Carlo (MCMC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b420076d-028b-44bc-ab58-4159a1b96c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab3c5a1-5cdb-4f9e-9c7f-af604b6de8fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Grid approximation\n",
    "\n",
    "The parameters (i.e the unobserved variables in our model, as opposed to data, the observed variables) are normally continuous, but we can discretise, and just evaluate equation 1 above at a bunch of different parameter values ($p$ above). This scales with the number of parameters, so becomes unfeasible quite quickly.\n",
    "\n",
    "In what follows, we are trying to estimate the proportion of water on a globe, $p$, by randomly sampling its surface (e.g. by chucking it and recording what our thumb is touching when we catch it). Equivalent to a weighted coin with p(Heads) = p. If W is the number of times water is recorded, L the number of land, and N the total samples, then we model:\n",
    "\n",
    "$ W \\sim \\text{Binomial} (N, p) $\n",
    "\n",
    "And take a uniform prior $p \\sim \\text{Uniform}(0,1)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4ecd3e-9eab-480a-a0df-58a2d810d2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observed data:\n",
    "num_waters = 6\n",
    "num_land = 3\n",
    "num_total = num_waters + num_land\n",
    "\n",
    "def grid_approximate_binomial(n: int, k: int, grid_size: int, prior: np.ndarray=None, plot=True) -> np.ndarray:\n",
    "    p_grid = np.linspace(0,1, grid_size)\n",
    "    # if prior is None, assume a uniform distribution over the grid.\n",
    "    if prior is None:\n",
    "        prior = np.ones(grid_size)\n",
    "    # evaluate the probability of our observed data given our model.\n",
    "    # binomial(n, p, k): (n choose k) * p^k * (1-p)^(n-k)\n",
    "    likelihood = scipy.stats.binom.pmf(n=n, k=k, p=p_grid)   \n",
    "    posterior_unscaled  = likelihood * prior \n",
    "    posterior = posterior_unscaled / posterior_unscaled.sum()\n",
    "    \n",
    "    if plot:\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.lineplot(x=p_grid, y=posterior, ax=ax, marker='o')\n",
    "        ax.set_xlim(p_grid.min(), p_grid.max())\n",
    "        plt.ylabel('posterior')\n",
    "        plt.xlabel('p')\n",
    "        plt.show()\n",
    "        \n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f257cb-c6fd-4d24-8edb-38a48d34ff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = grid_approximate_binomial(n=num_total, k=num_waters, grid_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4394e68b-3c88-4d20-b7d2-9d4836a9a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = grid_approximate_binomial(n=num_total, k=num_waters, grid_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d75c42-8c48-4f54-9109-ef3b25b42fb8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Quadratic approximation\n",
    "\n",
    "By making stronger assumptions we can handle more complicated models. Specifically, we assume that the region near the peak of the posterior is well-approximated by a Gaussian. The Gaussian is well-behaved and described by only the mean and the variance. This approximation is quadratic because the logarithm of the Gaussian is a quadratic function:\n",
    "\\begin{equation}\n",
    " P(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n",
    "\\end{equation}\n",
    "\n",
    "So to approximate the posterior of our model we find the posterior mode (with one of the many many optimizers available, e.g gradient descent), then fit a Gaussian to the mode by estimating the curvature of the region. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ae5f9b-e6d3-4f73-a0f0-c08f50ed6fec",
   "metadata": {},
   "source": [
    "The pymc code would look like:\n",
    "```python\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "# Data\n",
    "W = 6\n",
    "L = 3\n",
    "N = W + L\n",
    "\n",
    "# Model\n",
    "with pm.Model() as globe_model:\n",
    "    # Uniform prior\n",
    "    p = pm.Uniform('p', 0, 1)\n",
    "    \n",
    "    # Binomial likelihood\n",
    "    W_obs = pm.Binomial('W_obs', n=N, p=p, observed=W)\n",
    "    \n",
    "    # Perform the quadratic approximation\n",
    "    approx = pm.fit()\n",
    "\n",
    "    # Display the summary of the quadratic approximation\n",
    "    trace = approx.sample(1000)\n",
    "    print(az.summary(trace, kind='stats'))\n",
    "\n",
    "```\n",
    "\n",
    "It seems like this is doing variational inference (ADVI) - it says its fitting to a Gaussian but it might have more assumptions in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa6887a-8bc4-4912-9637-b9ee372390e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use py_quap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e1878f-d324-4722-b4df-1d50aa0cd94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "# Data\n",
    "W = 6\n",
    "L = 3\n",
    "N = W + L\n",
    "\n",
    "# Model\n",
    "with pm.Model() as globe_model:\n",
    "    # Uniform prior\n",
    "    p = pm.Uniform('p', 0, 1)\n",
    "    \n",
    "    # Binomial likelihood\n",
    "    W_obs = pm.Binomial('W_obs', n=N, p=p, observed=W)\n",
    "    \n",
    "    # Perform the quadratic approximation\n",
    "    approx = pm.fit()\n",
    "\n",
    "    # Display the summary of the quadratic approximation\n",
    "    trace = approx.sample(1000)\n",
    "    print(az.summary(trace, kind='stats'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac42ed5f-d6d6-4085-a16c-f917c599da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and compare\n",
    "x = np.linspace(0, 1, 1000)\n",
    "\n",
    "mu = np.mean(trace['p'])\n",
    "sigma = np.std(trace['p'])\n",
    "quap_posterior = scipy.stats.norm.pdf(x, mu, sigma)\n",
    "grid_size = 50\n",
    "# need to scale this by the bin width due to the discretization\n",
    "grid_posterior= grid_approximate_binomial(n=num_total, k=num_waters, grid_size=grid_size, plot=False) / (1/grid_size)  \n",
    "p_grid = np.linspace(0,1,grid_size)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(x=p_grid, y=grid_posterior, ax=ax, marker='o', label='Grid')\n",
    "sns.lineplot(x=x, y=quap_posterior, ax=ax, label='QUAP')\n",
    "ax.set_xlim(p_grid.min(), p_grid.max())\n",
    "plt.ylabel('posterior')\n",
    "plt.xlabel('p')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b55a13-16bd-42f1-8015-a06f1f600f55",
   "metadata": {},
   "source": [
    "NB in our case the analytical form of the posterior is known - it's a beta distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1531c65-cd73-43fc-9817-33c5131c64ee",
   "metadata": {},
   "source": [
    "## Markov chain monte carlo\n",
    "\n",
    "MCMC draws samples from the posterior distribution, then you can draw the posterior from the histogram of these samples, effectively. We then work directly with the samples rather than an approximation of the posterior. You can use e.g. the Metropolis algorithm to get the samples. Details are saved for Chapter 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa7ef8-9fc7-4df9-96ef-038787879560",
   "metadata": {},
   "source": [
    "## Solutions to exercies (spoilers)\n",
    "\n",
    "Don't view these if you're trying to learn anything for yourself, they can't be unseen!\n",
    "\n",
    "- 2E1: 2 and 4 (these are equivalent formulations\n",
    "- 2E2: 3\n",
    "- 2E3: 1 and 4 (again, equivalent)\n",
    "- 2E4: The frequentist would say the chance of landing on water given infinite throws. The Bayesian would say that 0.7 expresses our uncertainty in the outcome. While the result is always 1 or 0, 1 is more likely than 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae24e34-230d-4e94-883a-e76cd918f297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2M1:\n",
    "# Assuming uniform p, calculate the grid approximate posterior for the globe example given the following data.\n",
    "# 1. W,W,W\n",
    "_ = grid_approximate_binomial(n=3, k=3, grid_size=50, plot=True)\n",
    "# 2. WWWL\n",
    "_ = grid_approximate_binomial(n=4, k=3, grid_size=50, plot=True)\n",
    "# 3. LWWLWWW\n",
    "_ = grid_approximate_binomial(n=7, k=5, grid_size=50, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fde4c4-70d7-4190-acc7-59c52dd7f0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2M2:\n",
    "# assume a prior for p that is zero when p < 0.5, positive constant when p >= 0.5. Get the posteriors as above.\n",
    "grid_size = 50\n",
    "p_grid = np.linspace(0,1, grid_size)\n",
    "prior = np.where(p_grid < 0.5, 0, 2)\n",
    "# 1. W,W,W\n",
    "_ = grid_approximate_binomial(n=3, k=3, grid_size=grid_size, prior=prior, plot=True)\n",
    "# 2. WWWL\n",
    "_ = grid_approximate_binomial(n=4, k=3, grid_size=grid_size, prior=prior, plot=True)\n",
    "# 3. LWWLWWW\n",
    "_ = grid_approximate_binomial(n=7, k=5, grid_size=grid_size, prior=prior, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357fe812-a43f-4df9-91e3-599337382146",
   "metadata": {},
   "source": [
    "2M3: Given two globes, p(water | Earth) = 0.7, p(water | Mars) = 0. Given a random toss that produces a land observation, find\n",
    "    P(Earth | land).\n",
    " \n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    P(\\text{Earth} | \\text{land}) &= \\frac{P(\\text{land} |\\text{Earth}) P(\\text{Earth})} {P(\\text{land}|\\text{Earth}) P(\\text{Earth}) + P(\\text{land}|\\text{Mars}) P(\\text{Mars})} \\\\\n",
    "                    &= \\frac{0.3 \\times 0.5}{0.3 \\times 0.5 + 1 \\times 0.5} \\\\\n",
    "                    &= \\frac{0.15}{0.65} \\\\\n",
    "                    &= 0.23\n",
    "\\end{aligned}\n",
    "\\end{equation}             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3ec4cb-6b81-4a1c-85b4-6782cfccaed4",
   "metadata": {},
   "source": [
    "- 2M4: Three cards -one with two black sides, one with one black and one white, and one with two white. Shuffle the cards, draw one, place it flat on the table - the side facing up is black. What is the probability the other side is also black, using the counting method?\n",
    "\n",
    "There are three ways of drawing a card with a black facing up, two of those are from the double-black card, so the probability is 2/3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c67631b-370f-4226-859c-0e5fb0b4164d",
   "metadata": {},
   "source": [
    "- 2M5: Four cards, B/B, B/W, W/W, B/B. What is the probability of the underside being black, given the topside is?\n",
    "\n",
    "Five total ways of generating B topside, four of which have B underside.\n",
    "therefore P = 4/5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f494bac-e353-4fa4-b005-30529de2d78a",
   "metadata": {},
   "source": [
    "- 2M6: B/B, B/W, W/W. Now the draw probability is not uniform - for every way to pull the B/B card, there are 2 ways to pull the B/W and 3 ways to pull the W/W. What is the new probability of the underside being black, given that the topside is?\n",
    "\n",
    "total ways = 2 + 2, ways with black underside = 2, so P = 1/2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90eb457-0adc-4aa3-8261-707019ce7ac8",
   "metadata": {},
   "source": [
    "- 2M7: B/B, B/W, W/W as before. Now two cards are drawn. Face up black card, face up white card. What is the chance of the first card being B/B now?\n",
    "\n",
    "Total ways of generating card 1 then 2 - $2*3 + 1*2$ (card one then two or three, or card two then three)\n",
    "Total ways in which card 1 is B/B = $2*3$.\n",
    "Therefore P = 6/8 = 0.75\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ebc648-c504-4b18-be74-1a58d728a01c",
   "metadata": {},
   "source": [
    "- 2H1: Two species of panda bear. $P(\\text{twins} | A ) = 0.1$. $P(\\text{twins} | B) = 0.2$\n",
    "\n",
    "Given a panda has just given birth to twins, what is the probability the next birth will be twins?\n",
    "\n",
    "naively, P(twins) = P(twins|A)P(A) + P(twins|B)P(B) = $0.1*0.5 + 0.2*0.5 = 0.15$\n",
    "\n",
    "This doesn't use the information that we have one twin birth.\n",
    "\n",
    "P(A | prev twins) = P(prev twins | A) P(A) / P(prev twins) $= 0.1*0.5 / 0.15 = 1/3$\n",
    "ditto P(B | prev twins) $= 0.2*0.5/0.15 = 2/3$\n",
    "\n",
    "Now using these probabilities, P(twins) $= 0.1 * 1/3 + 0.2 * 2/3 = 1/6$\n",
    "\n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc01c586-6fae-4a00-8734-64d02917d280",
   "metadata": {},
   "source": [
    "- 2H2: As in 2H1. Find P(A | twins)\n",
    "\n",
    "Did this as part of 2H1 - probability is 1/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02441aad-6e8b-4a32-b25f-bcae9d92eb6e",
   "metadata": {},
   "source": [
    "- 2H3: As in 2H2, but now the second birth is a single infant.  What is P(A | singleton, twins)?\n",
    "\n",
    "Priors are P(A) = 1/3, P(B) = 2/3\n",
    "\n",
    "P(A | singleton) = P(singleton | A) * P(A) / P(singleton)\n",
    "\n",
    "                 $ = 0.9 * 1/3 / (0.9 * 1/3 + 0.8 * 2/3) = 0.36$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc9603a-d6b8-4db0-9c80-89f4934363cc",
   "metadata": {},
   "source": [
    "- 2H4: As in 2H3, but now we have a panda test.\n",
    "P(A, test says A) = 0.8\n",
    "P (B, test says B) = 0.65\n",
    "\n",
    "To start with, find P(A | test says A) with naive P(A) and P(B)\n",
    "\n",
    "\n",
    "P (A | test says A) = P(A, test says A) / P(test says A) $= 0.8 *0.5 / (0.8*0.5+0.35*0.5) = 0.7 $\n",
    "\n",
    "But with our birth information,\n",
    "\n",
    "P(A | test says A) $= 0.8*0.36 / (0.8*0.36 + 0.35 * (1-0.36)) =0.56$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3_10] *",
   "language": "python",
   "name": "conda-env-py3_10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
