{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4c031f5-b3ab-4065-9494-eb67e3ab25a0",
   "metadata": {},
   "source": [
    "# Chapter 6\n",
    "Issues when adding variables to a regression - multicollinearity, post-treatment bias, collider bias.\n",
    "\n",
    "To start with, demonstrate Berkson's paradox, showing that the act of selection generates a negative correlation between two independent variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ea0e19-4341-443e-8d98-95ffaa655845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz\n",
    "import matplotlib.pyplot as plt\n",
    "# import networkx as nx\n",
    "import numpy as np\n",
    "# import scipy.stats\n",
    "import seaborn as sns\n",
    "# import polars as pl (doesn't work with pymc yet)\n",
    "import pandas as pd \n",
    "import pymc as pm \n",
    "# import pybayes\n",
    "\n",
    "sns.set_style(\"white\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6374c6-d6b7-4c4f-ac34-62ffd6393a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, we have two variables, trustworthiness and newsworthiness, and they are uncorrelated. \n",
    "# We are a grant committee, and we take 200 proposals, and then select the ones in the top 10% by average score.\n",
    "\n",
    "num_proposals = 200\n",
    "selection_fraction = 0.1\n",
    "trustworthiness = np.random.normal(loc=0, scale=1, size=num_proposals)\n",
    "newsworthiness = np.random.normal(loc=0, scale=1, size=num_proposals)\n",
    "score = trustworthiness + newsworthiness\n",
    "winners = np.where(score > np.quantile(score, (1-selection_fraction)))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(trustworthiness, newsworthiness)\n",
    "plt.scatter(trustworthiness[winners], newsworthiness[winners], c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af8921-7862-4f24-b820-2fd13b76b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.corrcoef(trustworthiness[winners], newsworthiness[winners]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3620b54",
   "metadata": {},
   "source": [
    "## Multicollinearity\n",
    "\n",
    "When there is a strong association between two or more predictior variables, the posterior distribution will seem like none of your variables is reliably associated with the outcome, even when they all are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b83248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo - predicting height using the length of both legs.\n",
    "\n",
    "N = 100\n",
    "\n",
    "height = np.random.normal(loc=10, scale=2, size=N)\n",
    "leg_prop = np.random.uniform(low=0.4, high=0.5, size=N)  # leg length as proportion of total height\n",
    "leg_left = leg_prop*height + np.random.normal(loc=0, scale=0.02, size=N) \n",
    "leg_right = leg_prop*height + np.random.normal(loc=0, scale=0.02, size=N)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"height\": height,\n",
    "    \"leg_left\": leg_left,\n",
    "    \"leg_right\": leg_right\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5769b620",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1cdf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are predicting the beta coefficient to be around average height / average leg length = 10/(0.45*10) ~ 2.2  \n",
    "with pm.Model() as model_leg_lengths:\n",
    "    alpha = pm.Normal('alpha', mu=10, sigma=100)\n",
    "    beta_left = pm.Normal('beta_left', mu=2, sigma=10)\n",
    "    beta_right = pm.Normal('beta_right', mu=2, sigma=10)\n",
    "\n",
    "    sigma = pm.Exponential('sigma', 1)\n",
    "\n",
    "    mu = alpha + beta_left * df['leg_left'] + beta_right * df['leg_right']\n",
    "\n",
    "    height_obs = pm.Normal('height_obs', mu=mu, sigma=sigma, observed=df['height'])\n",
    "\n",
    "    trace =  pm.sample(1000, tune=1000) #, nuts_sampler='numpyro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50549821",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arviz.plot_trace(trace)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "summary = arviz.summary(trace)\n",
    "print(summary)\n",
    "\n",
    "arviz.plot_forest(trace)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77463198",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data=trace.posterior, x='beta_left', y='beta_right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b875dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to explain the wide wide posterior, we can look at the joint posterior distribution of beta_left and beta_right:\n",
    "arviz.plot_posterior(data=trace, var_names=['beta_left', 'beta_right'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ee8195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB - the sum of the two betas is much more sensibly behaved:\n",
    "posterior_beta_sum = trace.posterior['beta_left'] + trace.posterior['beta_right']\n",
    "arviz.plot_posterior(posterior_beta_sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
