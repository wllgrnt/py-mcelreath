{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a738137-3fd9-48ac-8f3e-a638b017610f",
   "metadata": {},
   "source": [
    "# Chapter 4\n",
    "\n",
    "Bayesian interpretations of linear regression, via some Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f362f-c317-43c1-9399-c3deeb3fbcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pymc as pm \n",
    "\n",
    "import pybayes\n",
    "\n",
    "sns.set_style(\"white\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b9957-e28b-4b53-9f35-8d73b25c564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -m -p arviz,matplotlib,numpy,scipy,seaborn,pandas,pymcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33103e69-caa8-4cb4-acb4-7428d035067d",
   "metadata": {},
   "source": [
    "\n",
    "Firstly, we simulate some random walks to show empirically (if further evidence were needed) that lots of things end up being Gaussian.\n",
    "\n",
    "Eg random walks via binomial (e.g. flip a coin, step forward if heads, backward if tails)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f110f7-404c-4632-9de4-8bf2ee6dcdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_walks = 1000\n",
    "n_steps = 1000\n",
    "\n",
    "walks = np.random.binomial(n=1, p=0.5, size=(n_walks, n_steps))\n",
    "walks[walks == 0] = -1\n",
    "\n",
    "paths = np.cumsum(walks, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i in range(n_walks):\n",
    "    plt.plot(paths[i, :], alpha=0.05)\n",
    "\n",
    "plt.title('Walks')\n",
    "plt.xlabel('Step number')\n",
    "plt.ylabel('Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408d056d-a3f4-43e0-bba6-a244ee61da79",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_position = paths[:, -1]\n",
    "pybayes.utils.hist(final_position)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180c6ee8-49c1-4cf8-a837-67fc8e962adc",
   "metadata": {},
   "source": [
    "## Grid-approximating our two-parameter model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c384338-c59f-48b5-b75a-cfc96855308a",
   "metadata": {},
   "source": [
    "We're going to do a regression on some height data. Our model will be:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "h_i &\\sim \\mathcal{N}(\\mu, \\sigma) \\\\\n",
    "\\mu &\\sim \\mathcal{N}(178, 20) \\\\\n",
    "\\sigma &\\sim \\text{Uniform}(0, 50)\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Here our likelihood is line one, and line two and three are sensibly chosen priors. We can check the sensibleness by plotting the priors, and then looking at what they imply, with a prior predictive simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d95c11-d6ed-454e-b8dd-898cf91349e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_mean = 178\n",
    "mu_sigma = 20\n",
    "p_grid_mu = np.linspace(100,250, 1000)\n",
    "mu_prior = scipy.stats.norm.pdf(p_grid_mu, loc= mu_mean, scale=mu_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3deabe-59c7-4595-8683-4b7331b51a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "pybayes.utils.plot_nicely(x_vals=p_grid_mu, y_vals=mu_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fcdf2f-34ac-4091-a8f4-bab7fb4440d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_low = 0\n",
    "sigma_high = 50\n",
    "p_grid_sigma = np.linspace(-5,55, 100)\n",
    "sigma_prior = scipy.stats.uniform.pdf(p_grid_sigma, loc=sigma_low, scale=sigma_high)\n",
    "# it is really weird that uniform uses loc and scale to mean these things.\n",
    "pybayes.utils.plot_nicely(x_vals=p_grid_sigma, y_vals=sigma_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458fc11d-a4b4-4372-a710-7f98bc8b7dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mu = np.random.normal(loc=mu_mean, scale=mu_sigma, size=10_000)\n",
    "sample_sigma = np.random.uniform(low=sigma_low, high=sigma_low, size=10_000)\n",
    "prior_h = np.random.normal(loc=sample_mu, scale=sample_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa97b844-171a-4481-80fc-4c653b35f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pybayes.utils.hist(prior_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bd0ff9-9e5e-4d45-8f10-abcb4781c661",
   "metadata": {},
   "source": [
    "The above is not the empirical distribution of H, its not even Gaussian. It's the distribution of relative plausibilities of different heights before we've seen the data. Next step is to grab the data and grid-approximate the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac3ec19-3b53-4ca1-b995-332dae1ca4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "howell = \"https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/Howell1.csv\"\n",
    "\n",
    "df = pd.read_csv(howell, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9f3dfb-e103-4ab9-a9d4-739ce9a4b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3747e56-dd49-4b52-959e-7bfd74358114",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38996394-95f6-4aaa-9da7-6968310e7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use adults here\n",
    "d2 = df[df.age >= 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e33f43-3b9a-47da-a025-e29ea62e0b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid-approximate using an algo to be explained later.\n",
    "mu_list = np.linspace(150, 160, 100)\n",
    "sigma_list = np.linspace(7,9, 100)  # why these values i do not know - i assume this is from the observed data.\n",
    "# all combos of sigma and mu\n",
    "post = pd.DataFrame({\n",
    "    'mu': np.tile(mu_list, len(sigma_list)),\n",
    "    'sigma': np.repeat(sigma_list, len(mu_list))\n",
    "}) \n",
    "\n",
    "# Calculate the log likelihoods\n",
    "def log_likelihood(row):\n",
    "    mu = row['mu']\n",
    "    sigma = row['sigma']\n",
    "    ll = np.sum(scipy.stats.norm.logpdf(d2['height'], mu, sigma))\n",
    "    return ll\n",
    "\n",
    "post['LL'] = post.apply(log_likelihood, axis=1)\n",
    "\n",
    "# Calculate the product of likelihood and priors\n",
    "post['prod'] = (post['LL'] + \n",
    "                scipy.stats.norm.logpdf(post['mu'], 178, 20) + \n",
    "                np.where((post['sigma'] >= 0) & (post['sigma'] <= 50), np.log(1/50), -np.inf))\n",
    "\n",
    "# Convert to probability\n",
    "max_prod = np.max(post['prod'])\n",
    "post['prob'] = np.exp(post['prod'] - max_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a812abdd-4906-41f7-927e-7aade41edacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=post, x='mu', y='sigma', weights='prob',  fill=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5052d0-09a9-46f3-bb82-09b5b9fb8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_d = post.pivot(index='mu', columns='sigma', values='prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226e94f1-1512-4f35-ba07-35bc727bc64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(two_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca6e827-efed-4124-8ecc-36f3e0c887d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the posterior, by sampling from the rows numbers proportionally to the probability and pulling the params.\n",
    "\n",
    "rows = np.random.choice(post.index, 10_000, replace=True, p=post.prob/post.prob.sum())\n",
    "\n",
    "\n",
    "sample = pd.DataFrame.from_dict({'mu': post.iloc[rows].mu,\n",
    "                                 'sigma': post.iloc[rows].sigma}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7441983-256d-4a31-a979-fd8213448e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202f7eba-6fb5-4622-9a62-42caa7740f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=sample, x='mu', y='sigma', alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f85935e-4669-41b1-91fa-f4c1b3792fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(sample.mu, binwidth=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ab105f-2797-4d97-9740-c53773802495",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(sample.sigma, binwidth=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85606b53-1a52-40b8-8d14-3396975fe502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall our priors for mu and sigma \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(p_grid_mu, mu_prior, label='prior')\n",
    "ax.set_ylabel('p')\n",
    "ax2 = ax.twinx()\n",
    "sns.histplot(sample.mu, binwidth=0.1, ax=ax2, label='posterior')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141c9d10-6fc9-4479-b8c8-e258b4ed3ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our posterior for mu has collapsed as a result of our observations.\n",
    "print(f'HDPI for mu:', arviz.hdi(sample.mu.values))\n",
    "print(f'HDPI for sigma:', arviz.hdi(sample.sigma.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462eaf34-440a-4c8b-8a9b-704d7b42c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = d2.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5b9ca0-2531-41d1-9039-652195ae1c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we repeat all the above but only using 20 of the heights from the dataset, we get:\n",
    "# grid-approximate using an algo to be explained later.\n",
    "mu_list = np.linspace(150, 170, 100)\n",
    "sigma_list = np.linspace(4,20, 100)  \n",
    "# all combos of sigma and mu\n",
    "post = pd.DataFrame({\n",
    "    'mu': np.tile(mu_list, len(sigma_list)),\n",
    "    'sigma': np.repeat(sigma_list, len(mu_list))\n",
    "}) \n",
    "\n",
    "# Calculate the log likelihoods\n",
    "def log_likelihood(row):\n",
    "    mu = row['mu']\n",
    "    sigma = row['sigma']\n",
    "    ll = np.sum(scipy.stats.norm.logpdf(d3['height'], mu, sigma))\n",
    "    return ll\n",
    "\n",
    "post['LL'] = post.apply(log_likelihood, axis=1)\n",
    "\n",
    "# Calculate the product of likelihood and priors\n",
    "post['prod'] = (post['LL'] + \n",
    "                scipy.stats.norm.logpdf(post['mu'], 178, 20) + \n",
    "                np.where((post['sigma'] >= 0) & (post['sigma'] <= 50), np.log(1/50), -np.inf))\n",
    "\n",
    "# Convert to probability\n",
    "max_prod = np.max(post['prod'])\n",
    "post['prob'] = np.exp(post['prod'] - max_prod)\n",
    "\n",
    "# sample\n",
    "rows = np.random.choice(post.index, 10_000, replace=True, p=post.prob/post.prob.sum())\n",
    "sample = pd.DataFrame.from_dict({'mu': post.iloc[rows].mu,\n",
    "                                 'sigma': post.iloc[rows].sigma}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1657d56-069c-4245-8c36-ecf5d105e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(sample.mu, binwidth=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0317f7-0c6b-4c1f-9f10-ba3f1fabdb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(sample.sigma, binwidth=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f428c4-478a-4f8e-b1a2-15b97f550447",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=sample, x='mu', y='sigma', alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7993e5-a089-4738-828b-261b05edfee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the stdev is notably less Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40d134d-599e-496e-9871-6bc667dae9db",
   "metadata": {},
   "source": [
    "## Moving to quadratic approximation\n",
    "\n",
    "In r this is all done in quap. We fit a quadratic to the maximum of the a posteriori distro and use that.\n",
    "\n",
    "The model again:\\begin{equation}\n",
    "\\begin{aligned}\n",
    "h_i &\\sim \\mathcal{N}(\\mu, \\sigma) \\\\\n",
    "\\mu &\\sim \\mathcal{N}(178, 20) \\\\\n",
    "\\sigma &\\sim \\text{Uniform}(0, 50)\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463ef338-3515-406f-ab62-8551bc7ce4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a02b2-4ace-4e92-94c7-e88c5a59b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make and use py-quap, this is doing MCMC sampling\n",
    "\n",
    "with pm.Model() as height_model:\n",
    "    # Uniform prior for sigma\n",
    "    mu = pm.Normal('mu', mu=178, sigma=20)\n",
    "    sigma = pm.Uniform('sigma', lower=0, upper=50)\n",
    "    \n",
    "    # Normal likelihood\n",
    "    height = pm.Normal('height', mu=mu, sigma=sigma, observed=d2.height)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70a9305-9551-4418-9c58-3acd45f0eb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "with height_model:\n",
    "    trace = pm.sample(1000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80829c6c-e903-4931-b10d-ec89432a0aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.plot_trace(trace)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa8c0b6-674f-42d1-af85-e303d2317a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arviz.summary(trace, kind='stats'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7cd90d-7572-45d2-9b1d-658844e8fc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what happens if we use a much tighter and more informative prior on mu?\n",
    "with pm.Model() as height_model_2:\n",
    "    # Uniform prior for sigma\n",
    "    mu = pm.Normal('mu', mu=178, sigma=0.1)\n",
    "    sigma = pm.Uniform('sigma', lower=0, upper=50)\n",
    "    \n",
    "    # Normal likelihood\n",
    "    height = pm.Normal('height', mu=mu, sigma=sigma, observed=d2.height)\n",
    "    \n",
    "    \n",
    "    trace_2 = pm.sample(1000, tune=1000)\n",
    "    \n",
    "arviz.plot_trace(trace_2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(arviz.summary(trace_2, kind='stats'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95f410-26cc-41fe-898c-02743649d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model is insistent that the mean is 178, and this disagrees with the data a lot, so the posterior for sigma changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ba484c-e86d-4f3b-a759-a0f3fe8cdb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling from the quadratic approximation\n",
    "# note that here we're already got samples, but pretend we used quap. Then the quadratic approximation \n",
    "# is a multi-dimensional Gaussian, specified by the means and covariance of our distro.\n",
    "trace_df = arviz.extract(trace, combined=True).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94483672-6709-4e9b-9f3b-cba4636f7bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_df[['mu', 'sigma']].cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d962ef-6a7d-4310-a999-436b6946e08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decompose into the variances for the params, and the correlation\n",
    "np.diag(trace_df[['mu', 'sigma']].cov())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547c7b1-9958-4a1a-a771-c31a89618df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_df[['mu', 'sigma']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1523917d-2503-4394-b1a0-4e49843fa6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this matrix shows that learning about mu tells us little about sigma, and vice versa - may not always be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d28ed4-48a1-4a7c-8bdf-98fcc2e5db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can extract vectors of values from the Gaussian, given this info.\n",
    "mean_values = trace_df.mean()[['mu', 'sigma']]\n",
    "quad_samples = scipy.stats.multivariate_normal.rvs(mean=mean_values, cov=trace_df[['mu', 'sigma']].cov(), size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea9c91f-c666-4afc-9402-a27bddc14621",
   "metadata": {},
   "outputs": [],
   "source": [
    "pybayes.utils.hist(quad_samples[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350b6875-4420-4372-9b70-7b5c69c13df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pybayes.utils.hist(quad_samples[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fefeca4-93d0-4d1a-8172-40ce38e6b38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.hdi(quad_samples[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657dae02-03cd-482b-90a1-0d4878b338b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.hdi(quad_samples[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b6c766-e950-4a6b-97bf-aca1aed9976e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predicting things\n",
    "\n",
    "We've fit a Gaussian to some heights. What we want to do is model how some predictor variables affect an outcome of interest.\n",
    "Here we'll use weight to predict height.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "h_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n",
    "\\mu_i &= \\alpha + \\beta(x_i - \\bar{x}) \\\\\n",
    "\\alpha &\\sim \\text{Normal}(178, 20) \\\\\n",
    "\\beta  &\\sim \\text{Normal}(0,10) \\\\\n",
    "\\sigma &\\sim \\text{Uniform}(0, 50)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Now the mean depends on each row $i$. And we no longer estimate $\\mu$ as a parameter, instead we construct it, assuming the linear model given. Note the lack of $\\sim$, the $\\mu_i$ is deterministic given the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb6dd86-5eac-46eb-9261-0a092b192dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=d2, x='weight', y='height')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae4fe1a-5d98-4a9a-bd17-04504e67ce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do our priors mean? We can do a prior predictive simulation\n",
    "\n",
    "N = 100\n",
    "alpha = np.random.normal(loc=178, scale=20, size=N)\n",
    "beta = np.random.normal(loc=0, scale=10, size=N)\n",
    "\n",
    "fig, ax= plt.subplots()\n",
    "x = np.linspace(30, 60, N)\n",
    "x_bar = d2.weight.mean()\n",
    "for a, b in zip(alpha, beta):\n",
    "    \n",
    "    ax.plot(x, [a + b*(i-x_bar) for i in x], alpha=0.1)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ed1c02-eafe-4d50-a881-7be9cb1f79e9",
   "metadata": {},
   "source": [
    "note this is very silly. Noone on Earth is <0 or > 300 cm tall. So use a new prior on beta:\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta  \\sim \\text{Log-Normal}(0,1) \\\\\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d0c33b-3da8-462a-814e-1394812b4720",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.random.lognormal(mean=0, sigma=1, size=10_000)\n",
    "sns.histplot(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b4dded-450d-452f-af58-34d88d6c5fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat our prior predictive simulation\n",
    "\n",
    "N = 100\n",
    "alpha = np.random.normal(loc=178, scale=20, size=N)\n",
    "beta = np.random.lognormal(mean=0, sigma=1, size=N)\n",
    "\n",
    "fig, ax= plt.subplots()\n",
    "x = np.linspace(30, 60, N)\n",
    "x_bar = d2.weight.mean()\n",
    "for a, b in zip(alpha, beta):\n",
    "    ax.plot(x, [a + b*(i-x_bar) for i in x], alpha=0.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e27414c-ecb2-479d-bdce-39d37092d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now generate the posterior, as before\n",
    "\n",
    "x_bar = d2.weight.mean()\n",
    "\n",
    "with pm.Model() as height_model_2:\n",
    "    # Uniform prior for sigma\n",
    "    alpha = pm.Normal('alpha', mu=178, sigma=20)\n",
    "    beta = pm.LogNormal('beta', mu=0, sigma=1)\n",
    "    sigma = pm.Uniform('sigma', lower=0, upper=50)\n",
    "\n",
    "    mu = alpha + beta*(d2.weight-x_bar)\n",
    "    \n",
    "    height=pm.Normal('height', mu=mu, sigma=sigma, observed=d2.height) \n",
    "    \n",
    "    trace_regr = pm.sample(1000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0791cf9-5a81-4ede-8a97-2d55e618dbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.plot_trace(trace_regr)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(arviz.summary(trace_regr, kind='stats'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acdb784-eff5-48b8-acaf-0e26a5690a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualising our posterior. To start with, look at the raw data and the posterior mean.\n",
    "trace_regr_df = trace_regr.posterior.to_dataframe()\n",
    "fix, ax = plt.subplots()\n",
    "ax.scatter(data=d2, x='weight', y='height', alpha=0.25)\n",
    "\n",
    "x = np.linspace(d2.weight.min(), d2.weight.max(), 100)\n",
    "alpha_mean = trace_regr_df.alpha.mean()\n",
    "beta_mean = trace_regr_df.beta.mean()\n",
    "y = alpha_mean + beta_mean*(x - x_bar)\n",
    "\n",
    "ax.plot(x, y)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ebb1b8-b2de-4430-be79-f0422ea7f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now add some sample lines to show the uncertainty in the parameter values\n",
    "trace_regr_df = trace_regr.posterior.to_dataframe()\n",
    "fix, ax = plt.subplots()\n",
    "ax.scatter(data=d2, x='weight', y='height', alpha=0.25)\n",
    "\n",
    "x = np.linspace(d2.weight.min(), d2.weight.max(), 100)\n",
    "alpha_mean = trace_regr_df.alpha.mean()\n",
    "beta_mean = trace_regr_df.beta.mean()\n",
    "y = alpha_mean + beta_mean*(x - x_bar)\n",
    "\n",
    "ax.plot(x, y, c='green')\n",
    "\n",
    "lines = trace_regr_df.sample(10)\n",
    "\n",
    "for _, (a, b, s) in lines.iterrows():\n",
    "    y = a + b*(x - x_bar)\n",
    "    ax.plot(x, y, c='green', alpha=.2)\n",
    "# for i in range(10):\n",
    "#     alpha\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e63425-4084-4f45-9a38-8c8ed2b2bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see, as the number of points we're inferring from increases, the uncertainty is reduced.\n",
    "\n",
    "def get_posterior_from_sample(input_df) -> pd.DataFrame:\n",
    "    x_bar = input_df.weight.mean()\n",
    "    with pm.Model() as height_model_3:\n",
    "        alpha = pm.Normal('alpha', mu=178, sigma=20)\n",
    "        beta = pm.LogNormal('beta', mu=0, sigma=1)\n",
    "        sigma = pm.Uniform('sigma', lower=0, upper=50)\n",
    "        mu = alpha + beta*(input_df.weight-x_bar)\n",
    "        height=pm.Normal('height', mu=mu, sigma=sigma, observed=input_df.height) \n",
    "        trace_regr = pm.sample(1000, tune=1000)\n",
    "    return trace_regr.posterior.to_dataframe()\n",
    "\n",
    "for num_points in [10, 10, 100, len(d2)]:\n",
    "    sub_df = d2[:num_points]\n",
    "    # now generate the posterior, as before\n",
    "    posterior = get_posterior_from_sample(sub_df)\n",
    "    fix, ax = plt.subplots()\n",
    "    ax.scatter(data=sub_df, x='weight', y='height', alpha=0.25)\n",
    "\n",
    "    x = np.linspace(30, 65, 100)\n",
    "    alpha_mean = posterior.alpha.mean()\n",
    "    beta_mean = posterior.beta.mean()\n",
    "    y = alpha_mean + beta_mean*(x - x_bar)\n",
    "    ax.plot(x, y, c='green')\n",
    "\n",
    "    lines = posterior.sample(10)\n",
    "    for _, (a, b, s) in lines.iterrows():\n",
    "        y = a + b*(x - x_bar)\n",
    "        ax.plot(x, y, c='green', alpha=.2)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f5d61-3a6f-4f37-9561-135827e2f23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do better by plotting the interval.\n",
    "\n",
    "# to start with, what's the distribution of the posterior mu at a fixed point (e.g. weight=50)?\n",
    "\n",
    "mu_at_50 = trace_regr_df.alpha + trace_regr_df.beta * (50 - x_bar)\n",
    "\n",
    "sns.kdeplot(mu_at_50)\n",
    "plt.xlabel('mu | weight=50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83516638-f6de-43e7-b90f-26c93ea09b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu has a distribution (Gaussian, since its inputs are all Gaussians). So we can work out the HPDI.\n",
    "arviz.hdi(mu_at_50.values, hdi_prob=0.89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f470062a-2f43-404c-8ca3-6c284001a527",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_regr_df.alpha.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf8439-2952-418f-97c9-c3a258eaa295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can draw this interval for each value of the weight\n",
    "x = np.linspace(d2.weight.min(), d2.weight.max(), 100)\n",
    "y= trace_regr_df.alpha.values[:, np.newaxis] + trace_regr_df.beta.values[:, np.newaxis] * (x - x_bar).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555c8e51-271b-48f1-b40c-7219c755454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "arviz.plot_hdi(x, y)\n",
    "ax.scatter(data=d2, x='weight', y='height', alpha=0.25)\n",
    "alpha_mean = trace_regr_df.alpha.mean()\n",
    "beta_mean = trace_regr_df.beta.mean()\n",
    "y = alpha_mean + beta_mean*(x - x_bar)\n",
    "ax.plot(x, y)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192509a4-0157-4658-9111-c1c2683f768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what we want, though, are prediction intervals for h, not for mu.\n",
    "\n",
    "# redo the sampling from the posterior, just for convenience\n",
    "x_bar = d2.weight.mean()\n",
    "with pm.Model() as model_simulate_h:\n",
    "    alpha = pm.Normal('alpha', mu=178, sigma=20)\n",
    "    beta = pm.LogNormal('beta', mu=0, sigma=1)\n",
    "    sigma = pm.Uniform('sigma', lower=0, upper=50)\n",
    "    mu = alpha + beta*(d2.weight-x_bar)\n",
    "    height=pm.Normal('height', mu=mu, sigma=sigma, observed=d2.height) \n",
    "    samples = pm.sample(1000, tune=1000)\n",
    "\n",
    "samples_df = samples.posterior.to_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef2bc2e-303b-4820-a82b-1d21a1bbb430",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcedf3d-279a-479c-ace5-140f24739121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could sample heights and get hpdis by drawing from the normal distribution with the rows from samples_df. But\n",
    "# its easier to use pymc's builtins. Let's do the former this time to dispel the magic.\n",
    "sim_weights = np.linspace(d2.weight.min(), d2.weight.max(), 100)\n",
    "\n",
    "hpdis = np.zeros((len(samples_df), len(sim_weights))) # [[] * len(sim_weights)]\n",
    "\n",
    "for i, sim_weight in enumerate(sim_weights):\n",
    "    mu = samples_df.alpha + samples_df.beta*(sim_weight-d2.weight.mean())  # 4000 mu values\n",
    "    heights = np.random.normal(loc=mu, scale=samples_df.sigma) # 4000 heights\n",
    "    hpdis[:, i] = heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dc2510-aefa-4a5f-880e-c7b4e3ef046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw everything\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# raw data\n",
    "ax.scatter(data=d2, x='weight', y='height', alpha=0.25, color='#7570b3')\n",
    "\n",
    "# MAP line\n",
    "x = np.linspace(d2.weight.min(), d2.weight.max(), 100)\n",
    "alpha_mean = trace_regr_df.alpha.mean()\n",
    "beta_mean = trace_regr_df.beta.mean()\n",
    "y = alpha_mean + beta_mean*(x - x_bar)\n",
    "ax.plot(x, y, color='#1b9e77')\n",
    "\n",
    "# HPDI for the line\n",
    "mus= trace_regr_df.alpha.values[:, np.newaxis] + trace_regr_df.beta.values[:, np.newaxis] * (x - x_bar).T\n",
    "arviz.plot_hdi(x, mus, hdi_prob=0.89, fill_kwargs={'alpha': 0.25, 'color': '#1b9e77'})\n",
    "\n",
    "# HPDI for simulated heights\n",
    "arviz.plot_hdi(x, hpdis, hdi_prob=0.89, fill_kwargs={'alpha': 0.1, 'color': '#1b9e77'})\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7617a9b5-37e0-40dd-a892-30b6eecead28",
   "metadata": {},
   "source": [
    "## Linear regression with curves\n",
    "\n",
    "We can use linear regression with more complicated models. E.g:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mu_i = \\alpha + \\beta_1 x_i - \\beta_2 x_i^2 \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c1e65-00a7-4257-808c-5f4fa59fd217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at our dataset, now including children. Note that it is absolutely not linear.\n",
    "df.plot.scatter(x='weight', y='height')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d52694-e8f1-4ad7-88fe-9c2ff5a4b038",
   "metadata": {},
   "source": [
    "### Parabolae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45363097-6bc5-4143-8eed-d5aafab3f654",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "h_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n",
    "\\mu_i &= \\alpha + \\beta_1 x_i - \\beta_2 x_i^2  \\\\\n",
    "\\alpha &\\sim \\text{Normal}(178, 20) \\\\\n",
    "\\beta_1  &\\sim \\text{Log-Normal}(0,1) \\\\\n",
    "\\beta_2 &\\sim \\text{Normal}(0,1) \\\\\n",
    "\\sigma &\\sim \\text{Uniform}(0, 50)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "The prior for $\\beta_2$ is somewhat arbitrarily assigned here. Prior predictive simulation will help guide the way (see practice problems at the end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7a0c39-b34a-4f55-a51b-28b18cf83e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximate the posterior, with standardised weights\n",
    "df['weight_s'] = (df.weight - df.weight.mean()) / df.weight.std()\n",
    "df['weight_s2'] = df.weight_s.pow(2)\n",
    "\n",
    "with pm.Model() as poly_model:\n",
    "    # priors\n",
    "    alpha = pm.Normal('alpha', mu=178, sigma=20)\n",
    "    beta1 = pm.LogNormal('beta1', mu=0, sigma=1)\n",
    "    beta2 = pm.Normal('beta2', mu=0, sigma=1)\n",
    "    sigma = pm.Uniform('sigma', lower=0, upper=50)\n",
    "    # model\n",
    "    mu = pm.Deterministic('mu', alpha + beta1 * df.weight_s + beta2 * df.weight_s2)\n",
    "    # likelihood\n",
    "    height=pm.Normal('height', mu=mu, sigma=sigma, observed=df.height) \n",
    "    poly_model_samples = pm.sample(1000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8490ae13-a21c-411f-9321-280e12bc7344",
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.plot_trace(poly_model_samples, var_names=['~mu'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5598590b-da29-4bcc-bce0-f9f4dcc7a11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.summary(poly_model_samples, var_names=['~mu'], hdi_prob=.89).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6584d00c-7351-40ac-8233-9ad2c15a4bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_model_posterior = poly_model_samples.posterior.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5e6925-032d-4749-b2d9-ee7a447a51b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_model_posterior.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a3cb6a-9b07-4aaa-a416-fa2150d80afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_pred = poly_model_samples.posterior['mu']\n",
    "\n",
    "height_pred = pm.sample_posterior_predictive(poly_model_samples, model=poly_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c4581c-3479-405b-8eac-728083af2986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mean relationship and the 89% intervals\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# raw data\n",
    "ax.scatter(data=df, x='weight_s', y='height', alpha=0.25, color='#7570b3')\n",
    "s\n",
    "# MAP line\n",
    "# num_x_vals = 100\n",
    "# x = np.linspace(df.weight_s.min(), df.weight_s.max(), num_x_vals)\n",
    "# alpha_mean = poly_model_posterior.alpha.mean()\n",
    "# beta1_mean = poly_model_posterior.beta1.mean()\n",
    "# beta2_mean = poly_model_posterior.beta2.mean()\n",
    "\n",
    "\n",
    "# ppd interval for the mean\n",
    "num_x_vals = 100\n",
    "x = np.linspace(df.weight_s.min(), df.weight_s.max(), num_x_vals)\n",
    "\n",
    "arviz.plot_hdi(df.weight_s, mu_pred, hdi_prob=0.89, fill_kwargs={'alpha': 0.25, 'color': '#1b9e77'})\n",
    "# ppd interval for the heights\n",
    "arviz.plot_hdi(df.weight_s, height_pred.posterior_predictive['height'], hdi_prob=0.89, fill_kwargs={'alpha': 0.1, 'color': '#1b9e77'})\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b0f4d-9563-4631-936d-b2ef53967c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial model definitely fits the sample data better than a linear model - does that make it a  better model? \n",
    "# are we learning anything causal from this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801ed964-7ef4-4d99-a014-dbeacee219b6",
   "metadata": {},
   "source": [
    "### Splines\n",
    "\n",
    "Specifically b-splines, using basis functions. We will look at cherry blossom data since it's wigglier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a558bc83-96b5-4c4a-9f02-e274cb7830a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cherry_blossom_url = \"https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/cherry_blossoms.csv\"\n",
    "cherry_blossoms = pd.read_csv(cherry_blossom_url, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6821b01-ce4d-43ce-ace6-9307dc924d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "cherry_blossoms.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec981b33-2874-45da-89cb-103e0d8fceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here doy is the day of the year of the first day of blossom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c75eb10-e5e3-4958-9eb5-1c9a0df40784",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "sns.scatterplot(data=cherry_blossoms, x='year', y='doy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b934751b-546c-4011-a881-e7306186b8bf",
   "metadata": {},
   "source": [
    "With B-splines, we generate new predictor variables and use those in our linear model $\\mu_i$, using basis functions $B_i$\n",
    "\n",
    "Our model then becomes:\n",
    "\\begin{equation}\n",
    "\\mu_i = \\alpha + w_1 B_{i1} + w_2 B_{i2} + ...\n",
    "\\end{equation}\n",
    "\n",
    "i.e. we have a w parameter for each basis function. Our basis variables can be constructed in a bunch of ways - here we use linear basis functions that turn off and on such that at any point only two basis functions are non-zero. We call the pivot points at which the basis functions peak the 'knots'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee8b503-62c2-4bd6-a5af-349d743eea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the knots/pivot points - here we use quantiles of the year\n",
    "cherry_blossoms_non_null = cherry_blossoms[cherry_blossoms.doy.notnull()]\n",
    "num_knots = 15\n",
    "knot_locations = cherry_blossoms_non_null.year.quantile(np.linspace(0, 1, num_knots)).values.astype(int)\n",
    "internal_knots = knot_locations[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d6947e-965e-42b0-b507-67e245b84879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this actually does that fitting - to do the matrix-style approach i'd need the 'patsy' library which I've never used before,\n",
    "# so i shan't.\n",
    "spline = scipy.interpolate.LSQUnivariateSpline(cherry_blossoms_non_null.year, cherry_blossoms_non_null.doy, t=internal_knots, k=3)\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "sns.scatterplot(data=cherry_blossoms, x='year', y='doy', alpha=0.25)\n",
    "plt.plot(cherry_blossoms_non_null.year, spline(cherry_blossoms_non_null.year))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eb3482-5e55-47b9-aede-aaefb90c495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spline = scipy.interpolate.LSQUnivariateSpline(cherry_blossoms_non_null.year, cherry_blossoms_non_null.doy, t=internal_knots, k=1)\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "sns.scatterplot(data=cherry_blossoms, x='year', y='doy', alpha=0.25)\n",
    "plt.plot(cherry_blossoms_non_null.year, spline(cherry_blossoms_non_null.year))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5332f5c1-8755-4df4-810b-ff60940e164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fit the model and get the posterior we'd need the actual basis functions, which sadly we do not have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35df529-cfd3-42c0-a918-b67973b43881",
   "metadata": {},
   "source": [
    "## Solutions to exercises\n",
    "\n",
    "Do the exercises yourself.\n",
    "\n",
    "- 4E1: line 1\n",
    "- 4E2: 2\n",
    "- 4E3: p(mu, sigma | y) = p(y | sigma, mu) p (mu | 0, 10) p (sigma | 1) / big interval\n",
    "- 4E4: 2\n",
    "- 4E5: 3 (mu is now deterministic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f14985-852a-4716-ad42-e8cf67d93ce6",
   "metadata": {},
   "source": [
    "4M1 - simulate observed y values from the prior\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "y_i &\\sim \\mathcal{N}(\\mu, \\sigma) \\\\\n",
    "\\mu &\\sim \\text{Normal}(0, 10) \\\\\n",
    "\\sigma &\\sim \\text{Exponential}(1)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b510c700-e1ea-452d-9c3f-1e41ccfcab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10_000\n",
    "mu = np.random.normal(loc=0, scale=10, size=num_samples)\n",
    "sigma = np.random.exponential(scale=1, size=num_samples)\n",
    "y = np.random.normal(mu, sigma)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(15,5))\n",
    "\n",
    "for sample, ax, name in zip([mu, sigma, y], axes, ['mu', 'sigma', 'y']):\n",
    "    sns.histplot(sample, ax=ax)\n",
    "    ax.set_title(name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197e58d1-fbb5-4ae0-93a9-afdf146c5a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4M2: as above, but with quap\n",
    "# (i will be using pymc4 because i haven't implemented pyquap yet\n",
    "with pm.Model() as model_4m2:\n",
    "    # priors\n",
    "    mu = pm.Normal('mu', mu=0, sigma=10)\n",
    "    sigma = pm.Exponential('sigma', lam=1)\n",
    "    # likelihood\n",
    "    y=pm.Normal('y', mu=mu, sigma=sigma) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36a84df-d602-494b-8b8e-3b4566706753",
   "metadata": {},
   "source": [
    "4M3\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "y_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n",
    "\\mu_i &= \\alpha + \\beta x \\\\\n",
    "\\alpha &\\sim \\text{Normal}(0, 10) \\\\\n",
    "\\beta &\\sim \\text{Uniform}(0, 1) \\\\\n",
    "\\sigma &\\sim \\text{Exponential}(1)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc2a2f6-6ae4-415f-8630-08d86509567b",
   "metadata": {},
   "source": [
    "**4M4: measure height each year for 3 years. Predict height using year as a predictor. What's the model definition?**\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "h_ij &\\sim \\mathcal{N}(\\mu_{}, \\sigma) \\\\\n",
    "\\mu_i &= \\alpha + \\beta (y_i - \\bar{y})  \\\\\n",
    "\\alpha &\\sim \\text{Normal}(100, 20) \\\\\n",
    "\\beta &\\sim \\text{Log-Normal}(1, 1) \\\\\n",
    "\\sigma &\\sim \\text{Exponential}(1)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "We could do prior predictives on this to see if it looked reasonable.\n",
    "\n",
    "**4M5: does knowing that every student gets taller each year change the priors?**\n",
    "\n",
    "No - this is factored into the prior choice for $\\beta$ already.\n",
    "\n",
    "**4M6: what about if you know that the variance among heights is never more than 64cm?**\n",
    "\n",
    "If we know $\\sigma^2 < 64$ then maybe a uniform distribution is better than an exponential, $\\sigma \\sim \\text{Uniform}(0, 8)$. What proportion of an exponential with scale 1 is >8, anyway?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a493619-9579-433f-8cc8-b17c80af3c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.random.exponential(scale=1, size=100_000)\n",
    "sns.histplot(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776e9992-e735-4a66-8512-4b220247ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_greater_than_8 = (samples > 8).sum()\n",
    "print(f'proportion of exponential distribution > 8: {num_greater_than_8/len(samples)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5390819e-a3f2-4f9b-92fd-d9ba0ae8939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4M7: refit m4.3, without including the mean weight $\\bar{x}$. Compare the posterior, and the covariance. Then look at the posterior predictions.\n",
    "# (expectation - they're the same, the covariance is higher because of the lack of scaling).\n",
    "\n",
    "with pm.Model() as model_4m7:\n",
    "    # priors\n",
    "    alpha = pm.Normal('alpha', mu=178, sigma=20)\n",
    "    beta = pm.LogNormal('beta', mu=0, sigma=1)\n",
    "    sigma = pm.Uniform('sigma', lower=0, upper=50)\n",
    "    # model\n",
    "    mu = pm.Deterministic('mu', alpha + beta * d2.weight)\n",
    "    # likelihood\n",
    "    height=pm.Normal('height', mu=mu, sigma=sigma, observed=d2.height) \n",
    "    model_4m7_samples = pm.sample(1000, tune=1000)\n",
    "    mean_q = pm.find_MAP()\n",
    "    hess = pm.find_hessian(mean_q, vars=[alpha, beta, sigma])    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f84c8c3-885f-46df-8961-b72ec8f68364",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_pred = model_4m7_samples.posterior['mu']\n",
    "height_pred = pm.sample_posterior_predictive(model_4m7_samples, model=model_4m7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e9cad9-2ac4-49ca-aa59-af2d007d1e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(d2.weight.min(), d2.weight.max(), 100)\n",
    "mu = mean_q['alpha'] + mean_q['beta'] * x\n",
    "\n",
    "plt.plot(x, mu)\n",
    "plt.scatter(d2.weight, d2.height, alpha=0.25)\n",
    "\n",
    "arviz.plot_hdi(d2.weight, mu_pred, hdi_prob=0.89, fill_kwargs={'alpha': 0.25, 'color': '#1b9e77'})\n",
    "# ppd interval for the heights\n",
    "arviz.plot_hdi(d2.weight, height_pred.posterior_predictive['height'], hdi_prob=0.89, fill_kwargs={'alpha': 0.1, 'color': '#1b9e77'})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7958b1d9-453f-460c-9327-3ebb8dbee2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "np.linalg.inv(hess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c702a2-d267-4ff2-8a95-0e9570a49dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(height_pred.posterior_predictive['height'].to_dataframe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc068bd-2f5b-477a-a9d1-5a804b31ccc2",
   "metadata": {},
   "source": [
    "(skipping the splines q, 4M*)\n",
    "\n",
    "4H1: given new data points, weights: 46.95, 43.72, 64.78, 32.59, 54.63. Give the expected height and 89% interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87fbe5a-8ae3-4d75-b965-dfdca4a44a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bar = d2.weight.mean()\n",
    "\n",
    "with pm.Model() as model_4h1:\n",
    "    # priors\n",
    "    alpha = pm.Normal('alpha', mu=178, sigma=20)\n",
    "    beta = pm.LogNormal('beta', mu=0, sigma=1)\n",
    "    sigma = pm.Uniform('sigma', lower=0, upper=50)\n",
    "    # model\n",
    "    mu = pm.Deterministic('mu', alpha + beta * (d2.weight - x_bar))\n",
    "    # likelihood\n",
    "    height=pm.Normal('height', mu=mu, sigma=sigma, observed=d2.height) \n",
    "    model_4h1_samples = pm.sample(1000, tune=1000)\n",
    "    map_h = pm.find_MAP()\n",
    "\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3e9c2-ce67-45f8-bbaf-e6ad4a2f34b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4h1_samples_df = model_4h1_samples.posterior.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4df008-82f7-4eec-951b-ae31e686a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = [46.95, 43.72, 64.78, 32.59, 54.63]\n",
    "fig, axes = plt.subplots(figsize=(5, 20), sharex=True, nrows=len(new_weights))\n",
    "new_weights_intervals = []\n",
    "new_weights_means = []\n",
    "for new_weight, ax in zip(new_weights, axes):\n",
    "    # sample the posterior. \n",
    "    mu = model_4h1_samples_df.alpha + model_4h1_samples_df.beta*(new_weight-d2.weight.mean())  # a bunch of possible mus\n",
    "    heights = np.random.normal(loc=mu, scale=model_4h1_samples_df.sigma)\n",
    "    sns.histplot(heights, ax=ax)\n",
    "    ax.set_title(f'Posterior prediction of heights for weight: {new_weight}')\n",
    "    # use mean as MAP approximation\n",
    "    print(f'hdi for {new_weight=}: {arviz.hdi(heights, hdi_prob=0.89)}')\n",
    "    print(f'mean of posterior for {new_weight=}: {heights.mean()}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7c9bb2-f3a9-442b-baa4-ebd8afda113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4H2 - look at only ages <= 18 in the Howell1 data. \n",
    "# a) fit a linear regression, present the estimates. For every 10 units of increase in weight, what is the predicted increase in height?\n",
    "# b) plot the raw data. Show the MAP line and the 89% interval for the mean, and predicted heights.\n",
    "# c) what's wrong with your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f30cd8d-98bf-441c-9150-d75a7113b67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "howell_children = df[df.age < 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd7f490-8a29-4195-a725-eb2ea45c3edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bar = howell_children.weight.mean()\n",
    "\n",
    "with pm.Model() as model_4h2:\n",
    "    # priors\n",
    "    alpha = pm.Normal('alpha', mu=100, sigma=20)\n",
    "    beta = pm.LogNormal('beta', mu=0, sigma=1)\n",
    "    sigma = pm.Uniform('sigma', lower=0, upper=50)\n",
    "    # model\n",
    "    mu = pm.Deterministic('mu', alpha + beta * (howell_children.weight - x_bar))\n",
    "    # likelihood\n",
    "    height=pm.Normal('height', mu=mu, sigma=sigma, observed=howell_children.height) \n",
    "    model_4h2_samples = pm.sample(1000, tune=1000)\n",
    "    map_vals = pm.find_MAP()\n",
    "\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c617b9bd-a899-4f1e-bcb0-f33b8b05c3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "float(map_vals['alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0874d4-b751-4d0f-bb3e-4096ba4f1ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('model alpha:', round(float(map_vals['alpha']), 2))\n",
    "print('model beta:', round(float(map_vals['beta']), 2))\n",
    "print('model sigma:', round(float(map_vals['sigma']), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2624cbb4-9089-4a49-a688-144ece32b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted height of the mean child - 108.31cm.\n",
    "# predicted increase in height for every 10kg in weight - 27.2cm\n",
    "# 95% of heights are expected to be within 2 sigma =  ~17cm of the trend line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099f78a3-24d9-4119-8e9f-f7365b59b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use to get the HPDIs\n",
    "mu_pred = model_4h2_samples.posterior['mu']\n",
    "height_pred = pm.sample_posterior_predictive(model_4h2_samples, model=model_4h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f53aa0-3f1d-4878-bc07-76e572dfbee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(howell_children.weight.min(), howell_children.weight.max(), 100)\n",
    "mu = map_vals['alpha'] + map_vals['beta'] * (x - x_bar)\n",
    "\n",
    "plt.plot(x, mu)\n",
    "plt.scatter(howell_children.weight, howell_children.height, alpha=0.25)\n",
    "\n",
    "arviz.plot_hdi(howell_children.weight, mu_pred, hdi_prob=0.89, fill_kwargs={'alpha': 0.25, 'color': '#1b9e77'})\n",
    "# ppd interval for the heights\n",
    "arviz.plot_hdi(howell_children.weight, height_pred.posterior_predictive['height'], hdi_prob=0.89, fill_kwargs={'alpha': 0.1, 'color': '#1b9e77'})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3852e0-bdea-48d6-bfed-a8794e712d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# above is clearly non-linear - uncertainty intervals are as a result very wide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e0607d-750e-4df5-a46b-773fb302f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4H3 - model height vs log (weight) - what are the coefficients, and what do they mean?\n",
    "with pm.Model() as model_4h3:\n",
    "    # priors\n",
    "    alpha = pm.Normal('alpha', mu=100, sigma=20)\n",
    "    beta = pm.Normal('beta', mu=0, sigma=10)\n",
    "    sigma = pm.Uniform('sigma', lower=0, upper=50)\n",
    "    # model\n",
    "    mu = pm.Deterministic('mu', alpha + beta * np.log(df.weight))\n",
    "    # likelihood\n",
    "    height=pm.Normal('height', mu=mu, sigma=sigma, observed=df.height) \n",
    "    model_4h3_samples = pm.sample(1000, tune=1000)\n",
    "    map_vals_4h3 = pm.find_MAP()\n",
    "\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d1549d-708c-4de5-baf0-e0afc9963827",
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.plot_trace(model_4h3_samples, var_names=['~mu'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f3b179-bfcb-4630-80f8-b75524b899eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('model alpha:', round(float(map_vals_4h3['alpha']), 2))\n",
    "print('model beta:', round(float(map_vals_4h3['beta']), 2))\n",
    "print('model sigma:', round(float(map_vals_4h3['sigma']), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4cebb4-dec3-4971-bdfd-c81ed444bf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a ~2.7x change in weight results in 46 additional height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a426c3a-3c77-45d2-9478-a359b6845d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use to get the HPDIs\n",
    "mu_pred = model_4h3_samples.posterior['mu']\n",
    "height_pred = pm.sample_posterior_predictive(model_4h3_samples, model=model_4h3)\n",
    "\n",
    "x = np.linspace(df.weight.min(), df.weight.max(), 100)\n",
    "mu = map_vals_4h3['alpha'] + map_vals_4h3['beta'] * np.log(x)\n",
    "plt.plot(x, mu, color='#1b9e77')\n",
    "plt.scatter(df.weight, df.height, alpha=0.25, s=2.5)\n",
    "\n",
    "arviz.plot_hdi(df.weight, mu_pred, hdi_prob=0.97, fill_kwargs={'alpha': 0.25, 'color': '#1b9e77'})\n",
    "# # ppd interval for the heights\n",
    "arviz.plot_hdi(df.weight, height_pred.posterior_predictive['height'], hdi_prob=0.97, fill_kwargs={'alpha': 0.1, 'color': '#1b9e77'})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01983cdd-8671-40ab-836e-4590833b0065",
   "metadata": {},
   "source": [
    "**4H4: plot the prior predictive distribution for the parabolic model in the chapter.**\n",
    "\n",
    "model was:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "h_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n",
    "\\mu_i &= \\alpha + \\beta_1 x_i + \\beta_2 x_i^2  \\\\\n",
    "\\alpha &\\sim \\text{Normal}(178, 20) \\\\\n",
    "\\beta_1  &\\sim \\text{Log-Normal}(0,1) \\\\\n",
    "\\beta_2 &\\sim \\text{Normal}(0,1) \\\\\n",
    "\\sigma &\\sim \\text{Uniform}(0, 50)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2674c19f-9b3c-406f-b3bb-87d8a85075e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines = 500\n",
    "alpha = alpha = np.random.normal(loc=178, scale=20, size=num_lines)\n",
    "beta_1 = np.random.lognormal(mean=0, sigma=1, size=num_lines)\n",
    "beta_2 = np.random.normal(loc=0, scale=1, size=num_lines)\n",
    "\n",
    "fig, ax= plt.subplots()\n",
    "\n",
    "num_points = 100\n",
    "x = np.linspace(d2.weight.min(), d2.weight.max(), num_points)\n",
    "\n",
    "for a, b1, b2 in zip(alpha, beta_1, beta_2):   \n",
    "    ax.plot(x, [a + b1*i + b2*i**2 for i in x], alpha=0.1)\n",
    "\n",
    "ax.set_ylabel('height')\n",
    "ax.set_xlabel('weight')\n",
    "ax.set_ylim([0, 300])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb55de0-4697-4698-b3a2-1d5619b39386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# above priors are busted - below are maybe more reasonable, keeping them loose.\n",
    "\n",
    "num_lines = 100\n",
    "alpha = alpha = np.random.normal(loc=-150, scale=10, size=num_lines)\n",
    "beta_1 = np.random.normal(loc=10, scale=1, size=num_lines)\n",
    "beta_2 = -np.random.lognormal(mean=-2.5, sigma=0.1, size=num_lines)\n",
    "\n",
    "fig, ax= plt.subplots()\n",
    "\n",
    "num_points = 100\n",
    "x = np.linspace(d2.weight.min(), d2.weight.max(), num_points)\n",
    "\n",
    "for a, b1, b2 in zip(alpha, beta_1, beta_2):   \n",
    "    ax.plot(x, [a + b1*i + b2*i**2 for i in x], alpha=0.1)\n",
    "\n",
    "ax.set_ylabel('height')\n",
    "ax.set_xlabel('weight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3_10] *",
   "language": "python",
   "name": "conda-env-py3_10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
