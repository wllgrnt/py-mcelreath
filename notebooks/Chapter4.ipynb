{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a738137-3fd9-48ac-8f3e-a638b017610f",
   "metadata": {},
   "source": [
    "# Chapter 4\n",
    "\n",
    "Bayesian interpretations of linear regression, via some Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f362f-c317-43c1-9399-c3deeb3fbcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pymc as pm \n",
    "\n",
    "import pybayes\n",
    "\n",
    "sns.set_style(\"white\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33103e69-caa8-4cb4-acb4-7428d035067d",
   "metadata": {},
   "source": [
    "\n",
    "Firstly, we simulate some random walks to show empirically (if further evidence were needed) that lots of things end up being Gaussian.\n",
    "\n",
    "Eg random walks via binomial (e.g. flip a coin, step forward if heads, backward if tails)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f110f7-404c-4632-9de4-8bf2ee6dcdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_walks = 1000\n",
    "n_steps = 1000\n",
    "\n",
    "walks = np.random.binomial(n=1, p=0.5, size=(n_walks, n_steps))\n",
    "walks[walks == 0] = -1\n",
    "\n",
    "paths = np.cumsum(walks, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i in range(n_walks):\n",
    "    plt.plot(paths[i, :], alpha=0.05)\n",
    "\n",
    "plt.title('Walks')\n",
    "plt.xlabel('Step number')\n",
    "plt.ylabel('Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408d056d-a3f4-43e0-bba6-a244ee61da79",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_position = paths[:, -1]\n",
    "pybayes.utils.hist(final_position)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180c6ee8-49c1-4cf8-a837-67fc8e962adc",
   "metadata": {},
   "source": [
    "## Grid-approximating our two-parameter model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c384338-c59f-48b5-b75a-cfc96855308a",
   "metadata": {},
   "source": [
    "We're going to do a regression on some height data. Our model will be:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "h_i &\\sim \\mathcal{N}(\\mu, \\sigma) \\\\\n",
    "\\mu &\\sim \\mathcal{N}(178, 20) \\\\\n",
    "\\sigma &\\sim \\text{Uniform}(0, 50)\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Here our likelihood is line one, and line two and three are sensibly chosen priors. We can check the sensibleness by plotting the priors, and then looking at what they imply, with a prior predictive simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d95c11-d6ed-454e-b8dd-898cf91349e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_mean = 178\n",
    "mu_sigma = 20\n",
    "p_grid_mu = np.linspace(100,250, 1000)\n",
    "mu_prior = scipy.stats.norm.pdf(p_grid_mu, loc= mu_mean, scale=mu_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3deabe-59c7-4595-8683-4b7331b51a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "pybayes.utils.plot_nicely(x_vals=p_grid_mu, y_vals=mu_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fcdf2f-34ac-4091-a8f4-bab7fb4440d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_low = 0\n",
    "sigma_high = 50\n",
    "p_grid_sigma = np.linspace(-5,55, 100)\n",
    "sigma_prior = scipy.stats.uniform.pdf(p_grid_sigma, loc=sigma_low, scale=sigma_high)\n",
    "# it is really weird that uniform uses loc and scale to mean these things.\n",
    "pybayes.utils.plot_nicely(x_vals=p_grid_sigma, y_vals=sigma_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458fc11d-a4b4-4372-a710-7f98bc8b7dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mu = np.random.normal(loc=mu_mean, scale=mu_sigma, size=10_000)\n",
    "sample_sigma = np.random.uniform(low=sigma_low, high=sigma_low, size=10_000)\n",
    "prior_h = np.random.normal(loc=sample_mu, scale=sample_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa97b844-171a-4481-80fc-4c653b35f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pybayes.utils.hist(prior_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bd0ff9-9e5e-4d45-8f10-abcb4781c661",
   "metadata": {},
   "source": [
    "The above is not the empirical distribution of H, its not even Gaussian. It's the distribution of relative plausibilities of different heights before we've seen the data. Next step is to grab the data and grid-approximate the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac3ec19-3b53-4ca1-b995-332dae1ca4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "howell = \"https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/Howell1.csv\"\n",
    "\n",
    "df = pd.read_csv(howell, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9f3dfb-e103-4ab9-a9d4-739ce9a4b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3747e56-dd49-4b52-959e-7bfd74358114",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38996394-95f6-4aaa-9da7-6968310e7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use adults here\n",
    "d2 = df[df.age >= 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e33f43-3b9a-47da-a025-e29ea62e0b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid-approximate using an algo to be explained later.\n",
    "mu_list = np.linspace(150, 160, 100)\n",
    "sigma_list = np.linspace(7,9, 100)  # why these values i do not know - i assume this is from the observed data.\n",
    "# all combos of sigma and mu\n",
    "post = pd.DataFrame({\n",
    "    'mu': np.tile(mu_list, len(sigma_list)),\n",
    "    'sigma': np.repeat(sigma_list, len(mu_list))\n",
    "}) \n",
    "\n",
    "# Calculate the log likelihoods\n",
    "def log_likelihood(row):\n",
    "    mu = row['mu']\n",
    "    sigma = row['sigma']\n",
    "    ll = np.sum(scipy.stats.norm.logpdf(d2['height'], mu, sigma))\n",
    "    return ll\n",
    "\n",
    "post['LL'] = post.apply(log_likelihood, axis=1)\n",
    "\n",
    "# Calculate the product of likelihood and priors\n",
    "post['prod'] = (post['LL'] + \n",
    "                scipy.stats.norm.logpdf(post['mu'], 178, 20) + \n",
    "                np.where((post['sigma'] >= 0) & (post['sigma'] <= 50), np.log(1/50), -np.inf))\n",
    "\n",
    "# Convert to probability\n",
    "max_prod = np.max(post['prod'])\n",
    "post['prob'] = np.exp(post['prod'] - max_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a812abdd-4906-41f7-927e-7aade41edacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=post, x='mu', y='sigma', weights='prob',  fill=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5052d0-09a9-46f3-bb82-09b5b9fb8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_d = post.pivot(index='mu', columns='sigma', values='prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226e94f1-1512-4f35-ba07-35bc727bc64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(two_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca6e827-efed-4124-8ecc-36f3e0c887d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the posterior, by sampling from the rows numbers proportionally to the probability and pulling the params.\n",
    "\n",
    "rows = np.random.choice(post.index, 10_000, replace=True, p=post.prob/post.prob.sum())\n",
    "\n",
    "\n",
    "sample = pd.DataFrame.from_dict({'mu': post.iloc[rows].mu,\n",
    "                                 'sigma': post.iloc[rows].sigma}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7441983-256d-4a31-a979-fd8213448e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202f7eba-6fb5-4622-9a62-42caa7740f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=sample, x='mu', y='sigma', alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f85935e-4669-41b1-91fa-f4c1b3792fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(sample.mu, binwidth=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ab105f-2797-4d97-9740-c53773802495",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(sample.sigma, binwidth=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85606b53-1a52-40b8-8d14-3396975fe502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall our priors for mu and sigma \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(p_grid_mu, mu_prior, label='prior')\n",
    "ax.set_ylabel('p')\n",
    "ax2 = ax.twinx()\n",
    "sns.histplot(sample.mu, binwidth=0.1, ax=ax2, label='posterior')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141c9d10-6fc9-4479-b8c8-e258b4ed3ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our posterior for mu has collapsed as a result of our observations.\n",
    "print(f'HDPI for mu:', arviz.hdi(sample.mu.values))\n",
    "print(f'HDPI for sigma:', arviz.hdi(sample.sigma.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462eaf34-440a-4c8b-8a9b-704d7b42c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = d2.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5b9ca0-2531-41d1-9039-652195ae1c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we repeat all the above but only using 20 of the heights from the dataset, we get:\n",
    "# grid-approximate using an algo to be explained later.\n",
    "mu_list = np.linspace(150, 170, 100)\n",
    "sigma_list = np.linspace(4,20, 100)  \n",
    "# all combos of sigma and mu\n",
    "post = pd.DataFrame({\n",
    "    'mu': np.tile(mu_list, len(sigma_list)),\n",
    "    'sigma': np.repeat(sigma_list, len(mu_list))\n",
    "}) \n",
    "\n",
    "# Calculate the log likelihoods\n",
    "def log_likelihood(row):\n",
    "    mu = row['mu']\n",
    "    sigma = row['sigma']\n",
    "    ll = np.sum(scipy.stats.norm.logpdf(d3['height'], mu, sigma))\n",
    "    return ll\n",
    "\n",
    "post['LL'] = post.apply(log_likelihood, axis=1)\n",
    "\n",
    "# Calculate the product of likelihood and priors\n",
    "post['prod'] = (post['LL'] + \n",
    "                scipy.stats.norm.logpdf(post['mu'], 178, 20) + \n",
    "                np.where((post['sigma'] >= 0) & (post['sigma'] <= 50), np.log(1/50), -np.inf))\n",
    "\n",
    "# Convert to probability\n",
    "max_prod = np.max(post['prod'])\n",
    "post['prob'] = np.exp(post['prod'] - max_prod)\n",
    "\n",
    "# sample\n",
    "rows = np.random.choice(post.index, 10_000, replace=True, p=post.prob/post.prob.sum())\n",
    "sample = pd.DataFrame.from_dict({'mu': post.iloc[rows].mu,\n",
    "                                 'sigma': post.iloc[rows].sigma}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1657d56-069c-4245-8c36-ecf5d105e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(sample.mu, binwidth=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0317f7-0c6b-4c1f-9f10-ba3f1fabdb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(sample.sigma, binwidth=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f428c4-478a-4f8e-b1a2-15b97f550447",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=sample, x='mu', y='sigma', alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7993e5-a089-4738-828b-261b05edfee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the stdev is notably less Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40d134d-599e-496e-9871-6bc667dae9db",
   "metadata": {},
   "source": [
    "## Moving to quadratic approximation\n",
    "\n",
    "In r this is all done in quap. We fit a quadratic to the maximum of the a posteriori distro and use that.\n",
    "\n",
    "The model again:\\begin{equation}\n",
    "\\begin{aligned}\n",
    "h_i &\\sim \\mathcal{N}(\\mu, \\sigma) \\\\\n",
    "\\mu &\\sim \\mathcal{N}(178, 20) \\\\\n",
    "\\sigma &\\sim \\text{Uniform}(0, 50)\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463ef338-3515-406f-ab62-8551bc7ce4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a02b2-4ace-4e92-94c7-e88c5a59b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make and use py-quap, this is doing MCMC sampling\n",
    "\n",
    "with pm.Model() as height_model:\n",
    "    # Uniform prior for sigma\n",
    "    mu = pm.Normal('mu', mu=178, sigma=20)\n",
    "    sigma = pm.Uniform('sigma', lower=0, upper=50)\n",
    "    \n",
    "    # Normal likelihood\n",
    "    height = pm.Normal('height', mu=mu, sigma=sigma, observed=d2.height)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70a9305-9551-4418-9c58-3acd45f0eb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "with height_model:\n",
    "    trace = pm.sample(1000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80829c6c-e903-4931-b10d-ec89432a0aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.plot_trace(trace)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa8c0b6-674f-42d1-af85-e303d2317a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arviz.summary(trace, kind='stats'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7cd90d-7572-45d2-9b1d-658844e8fc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what happens if we use a much tighter and more informative prior on mu?\n",
    "with pm.Model() as height_model_2:\n",
    "    # Uniform prior for sigma\n",
    "    mu = pm.Normal('mu', mu=178, sigma=0.1)\n",
    "    sigma = pm.Uniform('sigma', lower=0, upper=50)\n",
    "    \n",
    "    # Normal likelihood\n",
    "    height = pm.Normal('height', mu=mu, sigma=sigma, observed=d2.height)\n",
    "    \n",
    "    \n",
    "    trace_2 = pm.sample(1000, tune=1000)\n",
    "    \n",
    "arviz.plot_trace(trace_2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(arviz.summary(trace_2, kind='stats'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95f410-26cc-41fe-898c-02743649d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model is insistent that the mean is 178, and this disagrees with the data a lot, so the posterior for sigma changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ba484c-e86d-4f3b-a759-a0f3fe8cdb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling from the quadratic approximation\n",
    "# note that here we're already got samples, but pretend we used quap. Then the quadratic approximation \n",
    "# is a multi-dimensional Gaussian, specified by the means and covariance of our distro.\n",
    "trace_df = arviz.extract(trace, combined=True).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94483672-6709-4e9b-9f3b-cba4636f7bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_df[['mu', 'sigma']].cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d962ef-6a7d-4310-a999-436b6946e08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decompose into the variances for the params, and the correlation\n",
    "np.diag(trace_df[['mu', 'sigma']].cov())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547c7b1-9958-4a1a-a771-c31a89618df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_df[['mu', 'sigma']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1523917d-2503-4394-b1a0-4e49843fa6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this matrix shows that learning about mu tells us little about sigma, and vice versa - may not always be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d28ed4-48a1-4a7c-8bdf-98fcc2e5db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can extract vectors of values from the Gaussian, given this info.\n",
    "mean_values = trace_df.mean()[['mu', 'sigma']]\n",
    "quad_samples = scipy.stats.multivariate_normal.rvs(mean=mean_values, cov=trace_df[['mu', 'sigma']].cov(), size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea9c91f-c666-4afc-9402-a27bddc14621",
   "metadata": {},
   "outputs": [],
   "source": [
    "pybayes.utils.hist(quad_samples[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350b6875-4420-4372-9b70-7b5c69c13df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pybayes.utils.hist(quad_samples[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fefeca4-93d0-4d1a-8172-40ce38e6b38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.hdi(quad_samples[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657dae02-03cd-482b-90a1-0d4878b338b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.hdi(quad_samples[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b6c766-e950-4a6b-97bf-aca1aed9976e",
   "metadata": {},
   "source": [
    "## Predicting things\n",
    "\n",
    "We've fit a Gaussian to some heights. What we want to do is model how some predictor variables affect an outcome of interest.\n",
    "Here we'll use weight to predict height.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "h_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n",
    "\\mu_i &= \\alpha + \\beta(x_i - \\bar{x}) \\\\\n",
    "\\alpha &\\sim \\text{Normal}(178, 20) \\\\\n",
    "\\beta  &\\sim \\text{Normal}(0,10) \\\\\n",
    "\\sigma &\\sim \\text{Uniform}(0, 50)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Now the mean depends on each row $i$. And we no longer estimate $\\mu$ as a parameter, instead we construct it, assuming the linear model given. Note the lack of $\\sim$, the $\\mu_i$ is deterministic given the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb6dd86-5eac-46eb-9261-0a092b192dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=d2, x='weight', y='height')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae4fe1a-5d98-4a9a-bd17-04504e67ce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do our priors mean? We can do a prior predictive simulation\n",
    "\n",
    "N = 100\n",
    "alpha = np.random.normal(loc=178, scale=20, size=N)\n",
    "beta = np.random.normal(loc=0, scale=10, size=N)\n",
    "\n",
    "fig, ax= plt.subplots()\n",
    "x = np.linspace(30, 60, N)\n",
    "x_bar = d2.weight.mean()\n",
    "for a, b in zip(alpha, beta):\n",
    "    \n",
    "    ax.plot(x, [a + b*(i-x_bar) for i in x], alpha=0.1)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ed1c02-eafe-4d50-a881-7be9cb1f79e9",
   "metadata": {},
   "source": [
    "note this is very silly. Noone on Earth is <0 or > 300 cm tall. So use a new prior on beta:\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta  \\sim \\text{Log-Normal}(0,1) \\\\\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d0c33b-3da8-462a-814e-1394812b4720",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.random.lognormal(mean=0, sigma=1, size=10_000)\n",
    "sns.histplot(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b4dded-450d-452f-af58-34d88d6c5fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat our prior predictive simulation\n",
    "\n",
    "N = 100\n",
    "alpha = np.random.normal(loc=178, scale=20, size=N)\n",
    "beta = np.random.lognormal(mean=0, sigma=1, size=N)\n",
    "\n",
    "fig, ax= plt.subplots()\n",
    "x = np.linspace(30, 60, N)\n",
    "x_bar = d2.weight.mean()\n",
    "for a, b in zip(alpha, beta):\n",
    "    ax.plot(x, [a + b*(i-x_bar) for i in x], alpha=0.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e27414c-ecb2-479d-bdce-39d37092d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now generate the posterior, as before\n",
    "\n",
    "x_bar = d2.weight.mean()\n",
    "\n",
    "with pm.Model() as height_model_2:\n",
    "    # Uniform prior for sigma\n",
    "    alpha = pm.Normal('alpha', mu=178, sigma=20)\n",
    "    beta = pm.LogNormal('beta', mu=0, sigma=1)\n",
    "    sigma = pm.Uniform('sigma', lower=0, upper=50)\n",
    "\n",
    "    mu = alpha + beta*(d2.weight-x_bar)\n",
    "    \n",
    "    height=pm.Normal('height', mu=mu, sigma=sigma, observed=d2.height) \n",
    "    \n",
    "    trace_regr = pm.sample(1000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0791cf9-5a81-4ede-8a97-2d55e618dbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.plot_trace(trace_regr)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(arviz.summary(trace_regr, kind='stats'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acdb784-eff5-48b8-acaf-0e26a5690a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualising our posterior. To start with, look at the raw data and the posterior mean.\n",
    "trace_regr_df = trace_regr.posterior.to_dataframe()\n",
    "fix, ax = plt.subplots()\n",
    "ax.scatter(data=d2, x='weight', y='height', alpha=0.25)\n",
    "\n",
    "x = np.linspace(d2.weight.min(), d2.weight.max(), 100)\n",
    "alpha_mean = trace_regr_df.alpha.mean()\n",
    "beta_mean = trace_regr_df.beta.mean()\n",
    "y = alpha_mean + beta_mean*(x - x_bar)\n",
    "\n",
    "ax.plot(x, y)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ebb1b8-b2de-4430-be79-f0422ea7f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now add some sample lines to show the uncertainty in the parameter values\n",
    "trace_regr_df = trace_regr.posterior.to_dataframe()\n",
    "fix, ax = plt.subplots()\n",
    "ax.scatter(data=d2, x='weight', y='height', alpha=0.25)\n",
    "\n",
    "x = np.linspace(d2.weight.min(), d2.weight.max(), 100)\n",
    "alpha_mean = trace_regr_df.alpha.mean()\n",
    "beta_mean = trace_regr_df.beta.mean()\n",
    "y = alpha_mean + beta_mean*(x - x_bar)\n",
    "\n",
    "ax.plot(x, y, c='green')\n",
    "\n",
    "lines = trace_regr_df.sample(10)\n",
    "\n",
    "for _, (a, b, s) in lines.iterrows():\n",
    "    y = a + b*(x - x_bar)\n",
    "    ax.plot(x, y, c='green', alpha=.2)\n",
    "# for i in range(10):\n",
    "#     alpha\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e63425-4084-4f45-9a38-8c8ed2b2bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see, as the number of points we're inferring from increases, the uncertainty is reduced.\n",
    "\n",
    "def get_posterior_from_sample(input_df) -> pd.DataFrame:\n",
    "    x_bar = input_df.weight.mean()\n",
    "    with pm.Model() as height_model_3:\n",
    "        alpha = pm.Normal('alpha', mu=178, sigma=20)\n",
    "        beta = pm.LogNormal('beta', mu=0, sigma=1)\n",
    "        sigma = pm.Uniform('sigma', lower=0, upper=50)\n",
    "        mu = alpha + beta*(input_df.weight-x_bar)\n",
    "        height=pm.Normal('height', mu=mu, sigma=sigma, observed=input_df.height) \n",
    "        trace_regr = pm.sample(1000, tune=1000)\n",
    "    return trace_regr.posterior.to_dataframe()\n",
    "\n",
    "for num_points in [10, 10, 100, len(d2)]:\n",
    "    sub_df = d2[:num_points]\n",
    "    # now generate the posterior, as before\n",
    "    posterior = get_posterior_from_sample(sub_df)\n",
    "    fix, ax = plt.subplots()\n",
    "    ax.scatter(data=sub_df, x='weight', y='height', alpha=0.25)\n",
    "\n",
    "    x = np.linspace(30, 65, 100)\n",
    "    alpha_mean = posterior.alpha.mean()\n",
    "    beta_mean = posterior.beta.mean()\n",
    "    y = alpha_mean + beta_mean*(x - x_bar)\n",
    "    ax.plot(x, y, c='green')\n",
    "\n",
    "    lines = posterior.sample(10)\n",
    "    for _, (a, b, s) in lines.iterrows():\n",
    "        y = a + b*(x - x_bar)\n",
    "        ax.plot(x, y, c='green', alpha=.2)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f5d61-3a6f-4f37-9561-135827e2f23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do better by plotting the interval.\n",
    "\n",
    "# to start with, what's the distribution of the posterior mu at a fixed point (e.g. weight=50)?\n",
    "\n",
    "mu_at_50 = trace_regr_df.alpha + trace_regr_df.beta * (50 - x_bar)\n",
    "\n",
    "sns.kdeplot(mu_at_50)\n",
    "plt.xlabel('mu | weight=50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83516638-f6de-43e7-b90f-26c93ea09b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu has a distribution (Gaussian, since its inputs are all Gaussians). So we can work out the HPDI.\n",
    "arviz.hdi(mu_at_50.values, hdi_prob=0.89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f470062a-2f43-404c-8ca3-6c284001a527",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_regr_df.alpha.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf8439-2952-418f-97c9-c3a258eaa295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can draw this interval for each value of the weight\n",
    "x = np.linspace(d2.weight.min(), d2.weight.max(), 100)\n",
    "y= trace_regr_df.alpha.values[:, np.newaxis] + trace_regr_df.beta.values[:, np.newaxis] * (x - x_bar).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555c8e51-271b-48f1-b40c-7219c755454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "arviz.plot_hdi(x, y)\n",
    "ax.scatter(data=d2, x='weight', y='height', alpha=0.25)\n",
    "alpha_mean = trace_regr_df.alpha.mean()\n",
    "beta_mean = trace_regr_df.beta.mean()\n",
    "y = alpha_mean + beta_mean*(x - x_bar)\n",
    "ax.plot(x, y)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192509a4-0157-4658-9111-c1c2683f768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what we want, though, are prediction intervals for h, not for mu.\n",
    "\n",
    "\n",
    "x_bar = d2.weight.mean()\n",
    "with pm.Model() as model_simulate_h:\n",
    "    alpha = pm.Normal('alpha', mu=178, sigma=20)\n",
    "    beta = pm.LogNormal('beta', mu=0, sigma=1)\n",
    "    sigma = pm.Uniform('sigma', lower=0, upper=50)\n",
    "    mu = alpha + beta*(d2.weight-x_bar)\n",
    "    height=pm.Normal('height', mu=mu, sigma=sigma, observed=d2.height) \n",
    "    samples = pm.sample(1000, tune=1000)\n",
    "\n",
    "samples_df = samples.posterior.to_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53b4135-9b9e-4e93-a811-28d38b5228e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a916e796-6bef-4ada-9389-5f95f295549e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3_10] *",
   "language": "python",
   "name": "conda-env-py3_10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
